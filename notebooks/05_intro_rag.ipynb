{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86b7b3c1",
   "metadata": {},
   "source": [
    "# üìù 05 ‚Äì Retrieval & RAG Basics\n",
    "\n",
    "In this chapter, we‚Äôll learn about **RAG (Retrieval Augmented Generation)** and implement a simple RAG pipeline using LangChain with Gemini.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå 5.1 What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a technique that improves Large Language Models by combining:\n",
    "1. **Retrieval** ‚Üí fetching relevant information from an external knowledge base (vector DB).\n",
    "2. **Generation** ‚Üí feeding that information into the LLM to produce grounded, accurate responses.\n",
    "\n",
    "### Why RAG?\n",
    "- LLMs have limited **context windows** (they can only ‚Äúsee‚Äù a certain number of tokens at once).\n",
    "- LLMs can **hallucinate** (make up facts).\n",
    "- With RAG, the LLM always has **up-to-date, external information**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49651a4a",
   "metadata": {},
   "source": [
    "## üìå 5.2 Vector Databases\n",
    "\n",
    "To perform retrieval, we need a **vector database**:\n",
    "- Convert text into embeddings (numeric vectors).\n",
    "- Store embeddings in a database (Chroma, FAISS, Pinecone).\n",
    "- Perform similarity search to fetch most relevant chunks.\n",
    "\n",
    "We‚Äôll use **Chroma** here (easy, lightweight, local).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24f731c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca693bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM ready: ChatGoogleGenerativeAI\n"
     ]
    }
   ],
   "source": [
    "# initializing the llm\n",
    "from llm.load_llm import initialize_llm\n",
    "\n",
    "llm = initialize_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97689887",
   "metadata": {},
   "source": [
    "Pipeline:\n",
    "1. Convert docs ‚Üí embeddings (vectors)\n",
    "2. Store in a vector database\n",
    "3. Retrieve relevant chunks based on query\n",
    "4. Pass them + question into LLM\n",
    "\n",
    "We‚Äôll now build a simple demo with a few documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fb19fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Example documents\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain helps build apps with LLMs.\"),\n",
    "    Document(page_content=\"RAG means Retrieval Augmented Generation.\"),\n",
    "    Document(page_content=\"Chroma is a vector database for embeddings.\"),\n",
    "    Document(page_content=\"Embeddings convert text into numerical vectors.\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88fe107",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è 5.3. Split Documents into Chunks\n",
    "\n",
    "Large documents need to be **chunked** into smaller pieces for embeddings.\n",
    "We‚Äôll use `CharacterTextSplitter`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0531464b",
   "metadata": {},
   "source": [
    "Why use `CharacterTextSplitter`?\n",
    "\n",
    "* LLMs and embedding models have a token limit ‚Üí can‚Äôt process huge text at once.\n",
    "* Embedding small chunks helps better retrieval (more precise context).\n",
    "* CharacterTextSplitter breaks text into chunks of fixed size, usually by characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e28cf495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 4\n",
      "Chunk 1: LangChain helps build apps with LLMs.\n",
      "Chunk 2: RAG means Retrieval Augmented Generation.\n",
      "Chunk 3: Chroma is a vector database for embeddings.\n",
      "Chunk 4: Embeddings convert text into numerical vectors.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap = 10)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "print(\"Number of chunks:\" , len(split_docs))\n",
    "for i, d in enumerate(split_docs[:5]):\n",
    "    print(f\"Chunk {i+1}: {d.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14bd4624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 88, which is longer than the specified 50\n",
      "Created a chunk of size 78, which is longer than the specified 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tchunk:LangChain is a powerful framework for building applications using large language models.\n",
      "\n",
      "1\tchunk:It provides tools for chaining prompts, memory, agents, and retrieval systems.\n",
      "\n",
      "2\tchunk:RAG (Retrieval-Augmented Generation) is one of the most useful techniques in LangChain.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "\n",
    "data =  \"\"\"\n",
    "LangChain is a powerful framework for building applications using large language models.\n",
    "It provides tools for chaining prompts, memory, agents, and retrieval systems.\n",
    "RAG (Retrieval-Augmented Generation) is one of the most useful techniques in LangChain.\n",
    "\"\"\"\n",
    "text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap = 10, separator=\"\\n\")\n",
    "chunks = text_splitter.split_text(data)\n",
    "\n",
    "for i , c in enumerate(chunks):\n",
    "    print(f\"{i}\\tchunk:{c}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6599cba",
   "metadata": {},
   "source": [
    "## üî¢ 5.4. Create Embeddings and Store in Chroma\n",
    "\n",
    "Embeddings = numerical representation of text.  \n",
    "We‚Äôll use **Google Generative AI embeddings** (`models/embedding-001`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e09b77",
   "metadata": {},
   "source": [
    "What is an Embedding?\n",
    "\n",
    "* An embedding is a numerical representation of text.\n",
    "* Instead of words/characters, the text is turned into a vector (list of numbers) in high-dimensional space.0\n",
    "* Similar meanings ‚Üí closer vectors (small cosine distance).\n",
    "* Different meanings ‚Üí far apart vectors.\n",
    "\n",
    "üëâ This allows us to search by meaning, not just exact words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a24d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "vectordb = Chroma.from_documents(split_docs , embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d500e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "[0.04500666633248329, -0.008530666120350361, -0.035316307097673416, 0.02876937761902809, 0.052248261868953705, 0.017567235976457596, 0.03779364377260208, -0.0038489217404276133, 0.050953008234500885, 0.0346384271979332]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create embedding model\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# Example text\n",
    "text = \"LangChain makes it easier to build LLM apps.\"\n",
    "\n",
    "# Get embedding vector\n",
    "vector = embeddings.embed_query(text)\n",
    "print(len(vector))  # dimension size (e.g., 768 or more)\n",
    "print(vector[:10])  # first 10 values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797c39ba",
   "metadata": {},
   "source": [
    "## üîç 5.5. Create a Retriever\n",
    "\n",
    "Retriever = takes query ‚Üí finds top `k` most relevant docs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0062325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma is a vector database for embeddings.\n",
      "Embeddings convert text into numerical vectors.\n"
     ]
    }
   ],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\":2})\n",
    "\n",
    "results = retriever.get_relevant_documents(\"What is Chroma\")\n",
    "for r in results:\n",
    "    print(r.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3d0c4e",
   "metadata": {},
   "source": [
    "## ü§ñ 5.6. Build RAG QA Chain\n",
    "\n",
    "Now we connect:\n",
    "- Retriever (fetches context)\n",
    "- LLM (answers using context)\n",
    "\n",
    "We‚Äôll use `RetrievalQA`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55e700ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm2 = Ollama(model = \"tinyllama\")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm2,\n",
    "    retriever = retriever,\n",
    "    chain_type = \"stuff\",\n",
    "    verbose = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85a442c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Yes, I can provide the context for the question \"What is RAG and what is it used for?\"\n",
      "\n",
      "RAG (Retrieval Augmented Generation) is a type of AI-based machine learning model designed to improve natural language processing (NLP). The main purpose of RAG is to help the system understand and analyze large amounts of unstructured data, such as text or spoken dialogues, by providing contextual insights and interpretations.\n",
      "\n",
      "RAG uses embedding techniques to convert text into numerical vectors, allowing it to learn and understand contextual relationships between words and phrases within a given document or conversation. This helps the system better understand the underlying meaning and structure of the data, which can then be used to make more informed decisions and recommendations.\n"
     ]
    }
   ],
   "source": [
    "# Ask questions\n",
    "print(qa_chain.run(\"What does RAG mean\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd70c0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'query': 'Which database is used for embeddings', 'result': \"If you don't know the answer to this question, just skip to the next context piece.\\n\\nChroma is a vector database for embedding conversations and text into numerical vectors.\"}\n"
     ]
    }
   ],
   "source": [
    "print(qa_chain.invoke(\"Which database is used for embeddings\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83065c45",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "In this notebook we built a **basic RAG pipeline**:\n",
    "1. Prepared documents\n",
    "2. Split into chunks\n",
    "3. Created embeddings\n",
    "4. Stored in **Chroma vector DB**\n",
    "5. Retrieved relevant chunks\n",
    "6. Built a **RAG QA chain** with Gemini\n",
    "\n",
    "üëâ This is the foundation for **chat-with-PDF**, **Q&A bots**, and **knowledge assistants**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
