{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "673de081",
   "metadata": {},
   "source": [
    "# 3 Advanced Chains in LangChain\n",
    "## Topics\n",
    "* 3.1 Parallel Chains\n",
    "* 3.2 Branching Chains\n",
    "* 3.3 Output Parsers\n",
    "* 3.4 Memory in Chains\n",
    "* 3.5 Practice Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f908696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fc848fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM ready: ChatGoogleGenerativeAI\n"
     ]
    }
   ],
   "source": [
    "# initializing the llm\n",
    "from llm.load_llm import initialize_llm\n",
    "\n",
    "llm = initialize_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "260f1024",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing necessary modules\n",
    "from langchain.chains import LLMChain, SequentialChain, SimpleSequentialChain, ConversationChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8473c30",
   "metadata": {},
   "source": [
    "### 3.1 Parallel Chains\n",
    "Goal: for one input (topic) produce both a summary and a quiz question.\n",
    "We show two simple approaches: (A) run chains one after the other and collect results (simple & clear), (B) optional: do them concurrently using ThreadPoolExecutor (speedup if you want parallel HTTP calls)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773e9d69",
   "metadata": {},
   "source": [
    "#### 3.1.A Simple/Sequential Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67d3fe0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amalv\\AppData\\Local\\Temp\\ipykernel_18212\\343937080.py:5: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  summary_chain = LLMChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic': 'Finite Automata', 'summary': {'topic': 'Finite Automata', 'text': 'A diligent but simple machine, the Finite Automaton, follows a strict set of rules, reading input one symbol at a time and changing its state accordingly, ultimately accepting or rejecting the input based on its final state, much like a highly specialized vending machine only dispensing treats for specific code sequences.'}, 'quiz': {'topic': 'Finite Automata', 'text': 'Which of the following statements is TRUE regarding a Deterministic Finite Automaton (DFA)?\\n\\nA) A DFA can have multiple transitions from a single state on the same input symbol.\\nB) A DFA can accept an infinite number of strings.\\nC) A DFA always halts on every input string.\\nD) A DFA can have transitions that lead to no state (a \"dead end\").\\n\\n\\nAnswer: B'}}\n"
     ]
    }
   ],
   "source": [
    "summary_prompt = PromptTemplate.from_template(\"Wrtie a short story (1-2 sentence) summary about {topic}\")\n",
    "quiz_prompt = PromptTemplate.from_template(\"Write one quiz question about {topic} with correct answer on a new line labeled 'Answer:'\")\n",
    "\n",
    "## chains\n",
    "summary_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = summary_prompt\n",
    ")\n",
    "quiz_chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = quiz_prompt\n",
    ")\n",
    "# run them and collect results\n",
    "topic = \"Finite Automata\"\n",
    "summary = summary_chain.invoke({\"topic\":topic})\n",
    "quiz = quiz_chain.invoke({\"topic\":topic})\n",
    "\n",
    "result = {\"topic\": topic, \"summary\": summary, \"quiz\": quiz}\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86397368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"topic\": \"Finite Automata\",\n",
      "  \"summary\": {\n",
      "    \"topic\": \"Finite Automata\",\n",
      "    \"text\": \"A diligent but simple machine, the Finite Automaton, follows a strict set of rules, reading input one symbol at a time and changing its state accordingly, ultimately accepting or rejecting the input based on its final state, much like a highly specialized vending machine only dispensing treats for specific code sequences.\"\n",
      "  },\n",
      "  \"quiz\": {\n",
      "    \"topic\": \"Finite Automata\",\n",
      "    \"text\": \"Which of the following statements is TRUE regarding a Deterministic Finite Automaton (DFA)?\\n\\nA) A DFA can have multiple transitions from a single state on the same input symbol.\\nB) A DFA can accept an infinite number of strings.\\nC) A DFA always halts on every input string.\\nD) A DFA can have transitions that lead to no state (a \\\"dead end\\\").\\n\\n\\nAnswer: B\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(result , indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1054b4f",
   "metadata": {},
   "source": [
    "### 3.1.B Run in parallel(concurrent calls)\n",
    "* Runs chains concurrently. Useful when we want speed\n",
    "* thread-based concurrency calls both chains at the same time. Be mindful of rate limits and thread-safety for your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16609520",
   "metadata": {},
   "source": [
    "`with ThreadPoolExecutor(max_workers=2) as ex:`\n",
    "- Makes a pool of 2 threads (workers) that can do tasks at the same time.\n",
    "- ex is the executor that controls them.\n",
    "\n",
    "``` python\n",
    "futures = {}\n",
    "    for name, chain in chains:\n",
    "        future = ex.submit(run_chain , chain , inp)\n",
    "        futures[future] = name\n",
    "````\n",
    "* For each chain:\n",
    "    - ex.submit(run_chain, chain, inp) schedules the function run_chain(chain, inp) to run in a thread.\n",
    "    - Returns a Future object (a placeholder for the result).\n",
    "    - We store it in futures, mapping the future to the chain’s name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74063224",
   "metadata": {},
   "source": [
    "``` python\n",
    "parallel_result = {}\n",
    "for future, name in futures.items():\n",
    "    parallel_result[name] = future.result()\n",
    "```\n",
    "- Loop through each stored future.\n",
    "- future.result() waits for the thread to finish and returns the actual output from the chain.\n",
    "- Store it in a dictionary with the chain’s name as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d9b1fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"summary\": {\n",
      "    \"topic\": \"Reinforcement learning\",\n",
      "    \"text\": \"A resourceful robot, initially clumsy, learns to navigate a complex maze by trial and error, receiving rewards for successful moves and penalties for failures, gradually perfecting its path through repeated attempts and adapting its strategy based on the consequences of its actions.  This process, refined through countless iterations, exemplifies the core principles of reinforcement learning.\"\n",
      "  },\n",
      "  \"quiz\": {\n",
      "    \"topic\": \"Reinforcement learning\",\n",
      "    \"text\": \"Which of the following is NOT a core component of a Reinforcement Learning agent?\\n\\nA) Policy\\nB) Reward function\\nC) Value function\\nD) Supervised learning model\\n\\n\\nAnswer: D) Supervised learning model\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def run_chain(chain , inp):\n",
    "    return chain.invoke(inp)\n",
    "\n",
    "chains = [\n",
    "          (\"summary\" , summary_chain),\n",
    "          (\"quiz\" , quiz_chain)\n",
    "         ]\n",
    "inp = {\"topic\":\"Reinforcement learning\"}\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as ex:\n",
    "    futures = {}\n",
    "    for name, chain in chains:\n",
    "        future = ex.submit(run_chain , chain , inp)\n",
    "        futures[future] = name\n",
    "    \n",
    "    parallel_result = {}\n",
    "    for future, name in futures.items():\n",
    "        parallel_result[name] = future.result()\n",
    "\n",
    "print(json.dumps(parallel_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cecdbef",
   "metadata": {},
   "source": [
    "### 3.2 — Branching Chains (conditional flows)\n",
    "\n",
    "Goal: Inspect user input (or a classifier chain) and pick one of multiple chains to run.\n",
    "\n",
    "We’ll:\n",
    "- Use a tiny classifier prompt to pick a route (math, story, or general).\n",
    "- Based on the result run the appropriate chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffba68ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'route': 'math', 'output': {'q': 'Solve 12 * 7 + 5', 'text': '94'}}\n",
      "{'route': 'story', 'output': {'q': 'Write a cozy scene about a lost map found in an attic', 'text': \"Dust motes danced in the lone sunbeam slicing through the attic's gloom as Elara unearthed a rolled parchment.  Unfurling it carefully, she gasped – a hand-drawn map, faded but vibrant, depicting a winding path leading to a heart-shaped lake nestled amongst rolling hills.  The scent of old paper and forgotten adventures filled her senses, promising a journey far beyond the attic walls.\"}}\n"
     ]
    }
   ],
   "source": [
    "# Route chains\n",
    "math_prompt = PromptTemplate.from_template(\"Solve the math question: {q} and give the final numeric answer only.\")\n",
    "math_chain = LLMChain(llm=llm, prompt=math_prompt)\n",
    "\n",
    "story_prompt = PromptTemplate.from_template(\"Write a short (3-sentence) fictional scene for this prompt: {q}\")\n",
    "story_chain = LLMChain(llm=llm, prompt=story_prompt)\n",
    "\n",
    "general_prompt = PromptTemplate.from_template(\"Answer the question concisely: {q}\")\n",
    "general_chain = LLMChain(llm=llm, prompt=general_prompt)\n",
    "\n",
    "# Classifier: ask the model to decide which type it is\n",
    "classifier_prompt = PromptTemplate.from_template(\n",
    "    \"Classify the following user request into ONE of these single-word categories: math, story, general.\\n\\nRequest: \\\"{q}\\\"\\n\\nAnswer with exactly one word (math, story, or general).\"\n",
    ")\n",
    "classifier_chain = LLMChain(llm=llm, prompt=classifier_prompt)\n",
    "\n",
    "def route_and_run(q):\n",
    "    cat = classifier_chain.invoke({\"q\": q})[\"text\"].lower()\n",
    "    # normalize\n",
    "    cat = cat.split()[0] if cat else \"general\"\n",
    "    if \"math\" in cat:\n",
    "        return {\"route\": \"math\", \"output\": math_chain.invoke({\"q\": q})}\n",
    "    elif \"story\" in cat:\n",
    "        return {\"route\": \"story\", \"output\": story_chain.invoke({\"q\": q})}\n",
    "    else:\n",
    "        return {\"route\": \"general\", \"output\": general_chain.invoke({\"q\": q})}\n",
    "\n",
    "# Try a math prompt\n",
    "print(route_and_run(\"Solve 12 * 7 + 5\"))\n",
    "\n",
    "# Try a story prompt\n",
    "print(route_and_run(\"Write a cozy scene about a lost map found in an attic\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c03649",
   "metadata": {},
   "source": [
    "this manual routing pattern works well and is transparent for debugging. For production you can make the classifier more strict or use deterministic heuristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4601458f",
   "metadata": {},
   "source": [
    "### 3.3 - Output Parsers (JSON structured ouptuts)\n",
    "\n",
    "Ask the LLM to return JSON, then parse it into structured data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c96d3c",
   "metadata": {},
   "source": [
    "#### 3.3.A Use JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ede9c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw response:\n",
      " ```json\n",
      "{\n",
      "  \"name\": \"eco-friendly water bottle\",\n",
      "  \"tagline\": \"Hydrate sustainably.\"\n",
      "}\n",
      "```\n",
      "JSON parsing failed. Response was not valid JSON. Consider asking model to return valid JSON or use a stricter parser.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant that returns strict JSON only. For a product named '{product}', return a JSON object with keys:\\n\"\n",
    "    Return the result ONLY in valid JSON with this format:\n",
    "    {{\n",
    "      \"name\": \"...\",\n",
    "      \"tagline\": \"...\"\n",
    "    }}\"\"\"\n",
    ")\n",
    "\n",
    "json_chain = LLMChain(llm=llm, prompt=json_prompt)\n",
    "\n",
    "response_text = json_chain.invoke({\"product\": \"eco-friendly water bottle\"})\n",
    "print(\"Raw response:\\n\", response_text[\"text\"])\n",
    "\n",
    "# Try to parse\n",
    "try:\n",
    "    parsed = json.loads(response_text[\"text\"])\n",
    "    print(\"Parsed JSON:\", parsed)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"JSON parsing failed. Response was not valid JSON. Consider asking model to return valid JSON or use a stricter parser.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e282555",
   "metadata": {},
   "source": [
    "#### 3.3.B Use LangChains built-in JSON parser\n",
    "LangChain has a tool called StructuredOutputParser that ensures proper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9e204a",
   "metadata": {},
   "source": [
    "`ResponseSchema`\n",
    "- A way to define the fields you want from the LLM output.\n",
    "- Example: \"name\" and \"tagline\" are required keys.\n",
    "- You also add descriptions, so the model knows what kind of text to fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dbbdc6",
   "metadata": {},
   "source": [
    "`StructuredOutputParser`\n",
    "\n",
    "- Takes your schema and enforces it.\n",
    "- It generates the formatting instructions (get_format_instructions()) to tell the model exactly how to return results.\n",
    "- It also parses the model’s raw text back into a Python dict you can use directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359d4e10",
   "metadata": {},
   "source": [
    "`ChatPromptTemplate`\n",
    "\n",
    "- Builds prompt templates that work with chat models (like Gemini or GPT).\n",
    "- Lets you use placeholders ({format_instructions}, {product}, etc.).\n",
    "- Makes it easy to pass dynamic inputs into your prompts.\n",
    "- Unlike PromptTemplate, which is mostly for simple string prompts, ChatPromptTemplate is built for multi-turn, chat-style LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5126b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"AquaFlow Dynamics\",\n",
      "  \"tagline\": \"Hydration Perfected.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "schemas = [\n",
    "    ResponseSchema(name=\"name\", description=\"Company name\"),\n",
    "    ResponseSchema(name=\"tagline\", description=\"conpany tagline\")\n",
    "]\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schemas(schemas)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Suggest a company name for a {type} manufacturing company and tagexplaline\n",
    "    {format_instructions}\n",
    "\"\"\")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "res = chain.invoke({\"type\":\"water bottle\" , \"format_instructions\": format_instructions})\n",
    "print(json.dumps(res , indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca125ebf",
   "metadata": {},
   "source": [
    "### 3.4 -- Memory in Chains\n",
    "* Component that stores past interactions between the user and LLM\n",
    "* So, when we call the chain again, the LLM uses the memory to gain context\n",
    "* Used in chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad163292",
   "metadata": {},
   "source": [
    "#### 3.4.A \n",
    "`ConversationBufferMemory`\n",
    "* Stores the entire conversation history as text\n",
    "* Every question will include the history in prompt\n",
    "* Simple and great for shor chats/demos\n",
    "* If conversation is long, the history string may exceed LLM's context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5df6d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amalv\\AppData\\Local\\Temp\\ipykernel_18212\\3210456726.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key = \"chat_history\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You are a helpful assistant that answers in one word.\n",
      "    The conversation so far: \n",
      " \n",
      "\n",
      "    User: Who won the fifa world cup in 2022?\n",
      "    Assistant: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'user_input': 'Who won the fifa world cup in 2022?', 'chat_history': '', 'text': 'Argentina'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You are a helpful assistant that answers in one word.\n",
      "    The conversation so far: \n",
      " Human: Who won the fifa world cup in 2022?\n",
      "AI: Argentina\n",
      "\n",
      "    User: Who was the captain of that team\n",
      "    Assistant: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'user_input': 'Who was the captain of that team', 'chat_history': 'Human: Who won the fifa world cup in 2022?\\nAI: Argentina', 'text': 'Messi'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "#initialize the memory\n",
    "memory = ConversationBufferMemory(memory_key = \"chat_history\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\" , \"user_input\"],\n",
    "    template=\"\"\" You are a helpful assistant that answers in one word.\n",
    "    The conversation so far: \\n {chat_history}\\n\n",
    "    User: {user_input}\n",
    "    Assistant: \"\"\"\"\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt = prompt,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "print(chain.invoke({\"user_input\":\"Who won the fifa world cup in 2022?\"}))\n",
    "print(chain.invoke({\"user_input\":\"Who was the captain of that team\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32664fa2",
   "metadata": {},
   "source": [
    "`ConversationBufferWindowMemory`\n",
    "\n",
    "* Similar to ConversationBufferMemory, but only keeps the last k exchanges.\n",
    "* Example: keep only the last 3 turns.\n",
    "* ✅ Best for: long conversations where only recent context matters.\n",
    "* ❌ Loses old information.\n",
    "``` python\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "memory = ConversationBufferWindowMemory(k=3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb039e09",
   "metadata": {},
   "source": [
    "`ConversationSummaryMemory`\n",
    "\n",
    "* Instead of keeping everything, it summarizes old messages using an LLM.\n",
    "* The summary + recent conversation gets sent to the model.\n",
    "* ✅ Best for: very long conversations (e.g., customer support chatbots).\n",
    "* ❌ Summary may sometimes miss small details.\n",
    "```python\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc2238a",
   "metadata": {},
   "source": [
    "`ConversationSummaryBufferMemory`\n",
    "\n",
    "* Hybrid approach:\n",
    "* Keeps recent exchanges verbatim.\n",
    "* Older ones get summarized.\n",
    "* ✅ Best for: balance between context accuracy and size limits.\n",
    "* ❌ More complex, slightly slower (needs summarization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e9c31f",
   "metadata": {},
   "source": [
    "`VectorStoreRetrieverMemory`\n",
    "\n",
    "* Stores conversation history (or facts) in a vector database (like FAISS, Pinecone, Weaviate).\n",
    "* Retrieves the most relevant past exchanges when needed.\n",
    "* ✅ Best for: chatbots that need knowledge recall from long histories.\n",
    "* ❌ Requires setting up a vector DB.\n",
    "\n",
    "`EntityMemory`\n",
    "\n",
    "* Tracks entities (people, places, organizations, etc.) mentioned in the conversation.\n",
    "* Lets the model recall facts about entities across chats.\n",
    "* ✅ Best for: assistants where remembering specific people/items is important.\n",
    "* ❌ More specialized, needs tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef327fd",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "A simple `ConversationBufferMemory` to remember your name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d41cbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amalv\\AppData\\Local\\Temp\\ipykernel_18212\\4207493316.py:6: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(llm =llm, memory = memory, verbose= True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[]\n",
      "Human: Hi, my name is Amal.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Hi, my name is Amal.', 'history': [HumanMessage(content='Hi, my name is Amal.', additional_kwargs={}, response_metadata={}), AIMessage(content='Hi Amal! It\\'s nice to meet you.  My name isn\\'t really a \"name\" in the human sense, as I don\\'t have a personal identity like you do. I\\'m a large language model, trained by Google.  I don\\'t have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really well-read, incredibly fast librarian who can also write stories and answer your questions based on what I\\'ve learned.  So, what can I help you with today?  Are you interested in a specific topic, or do you just want to chat?', additional_kwargs={}, response_metadata={})], 'response': 'Hi Amal! It\\'s nice to meet you.  My name isn\\'t really a \"name\" in the human sense, as I don\\'t have a personal identity like you do. I\\'m a large language model, trained by Google.  I don\\'t have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really well-read, incredibly fast librarian who can also write stories and answer your questions based on what I\\'ve learned.  So, what can I help you with today?  Are you interested in a specific topic, or do you just want to chat?'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is Amal.', additional_kwargs={}, response_metadata={}), AIMessage(content='Hi Amal! It\\'s nice to meet you.  My name isn\\'t really a \"name\" in the human sense, as I don\\'t have a personal identity like you do. I\\'m a large language model, trained by Google.  I don\\'t have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really well-read, incredibly fast librarian who can also write stories and answer your questions based on what I\\'ve learned.  So, what can I help you with today?  Are you interested in a specific topic, or do you just want to chat?', additional_kwargs={}, response_metadata={})]\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'What is my name?', 'history': [HumanMessage(content='Hi, my name is Amal.', additional_kwargs={}, response_metadata={}), AIMessage(content='Hi Amal! It\\'s nice to meet you.  My name isn\\'t really a \"name\" in the human sense, as I don\\'t have a personal identity like you do. I\\'m a large language model, trained by Google.  I don\\'t have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really well-read, incredibly fast librarian who can also write stories and answer your questions based on what I\\'ve learned.  So, what can I help you with today?  Are you interested in a specific topic, or do you just want to chat?', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Amal.  You told me that at the beginning of our conversation.', additional_kwargs={}, response_metadata={})], 'response': 'Your name is Amal.  You told me that at the beginning of our conversation.'}\n",
      "\n",
      "---Memory Buffer---\n",
      "\n",
      "[HumanMessage(content='Hi, my name is Amal.', additional_kwargs={}, response_metadata={}), AIMessage(content='Hi Amal! It\\'s nice to meet you.  My name isn\\'t really a \"name\" in the human sense, as I don\\'t have a personal identity like you do. I\\'m a large language model, trained by Google.  I don\\'t have feelings or experiences, but I can access and process information from a massive dataset of text and code.  Think of me as a really well-read, incredibly fast librarian who can also write stories and answer your questions based on what I\\'ve learned.  So, what can I help you with today?  Are you interested in a specific topic, or do you just want to chat?', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Amal.  You told me that at the beginning of our conversation.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "#Setup memory and a conversation chain\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "chain = ConversationChain(llm =llm, memory = memory, verbose= True)\n",
    "\n",
    "print(chain.invoke(\"Hi, my name is Amal.\"))\n",
    "print(chain.invoke(\"What is my name?\"))\n",
    "\n",
    "print(\"\\n---Memory Buffer---\\n\")\n",
    "print(memory.buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
