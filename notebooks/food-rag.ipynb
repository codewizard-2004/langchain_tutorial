{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6411d0d2",
   "metadata": {},
   "source": [
    "### Creating a RAG system where\n",
    "* I create a bunch of data for food ingredients, nutrition facts, preparation, with youtube link\n",
    "* Chunk them and load them into chromadb\n",
    "* create a retrieval system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5511e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35f9e399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\langchain_tutorial\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "class LLMSettings:\n",
    "    def __init__(self) -> None:\n",
    "        sys.path.append(os.path.abspath(\"..\"))\n",
    "        self.gemini_api = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "        self.tavily_api_key = os.environ.get(\"TAVILY_API_KEY\")\n",
    "        \n",
    "    def load_gemini(self, temp: float = 0.5) -> ChatGoogleGenerativeAI:\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "            model = \"gemini-2.5-flash\",\n",
    "            api_key = self.gemini_api,\n",
    "            temperature = temp\n",
    "        )\n",
    "        print(\"LLM ready:\", type(llm).__name__)\n",
    "        return llm\n",
    "    \n",
    "    def load_local(self,model_id: int = 0, temp: float = 0.5)->ChatOpenAI:\n",
    "        \"\"\"\n",
    "        This method returns the local model hosted by LM Studio.\n",
    "        0 - google/gemma-3-4b\n",
    "        1 - microsoft/phi-4-mini-reasoning\n",
    "        2 - llama-3.2-1b-instruct\n",
    "        \"\"\"\n",
    "        model_map = {\n",
    "            0: \"google/gemma-3-4b\",\n",
    "            1: \"microsoft/phi-4-mini-reasoning\",\n",
    "            2: \"llama-3.2-1b-instruct\"\n",
    "        }\n",
    "        llm = ChatOpenAI(\n",
    "            model=model_map[model_id],\n",
    "            openai_api_key = 'lm-studio', # type: ignore\n",
    "            openai_api_base=\"http://localhost:1234/v1\", # type: ignore\n",
    "            temperature=temp\n",
    "        )\n",
    "        return llm\n",
    "    \n",
    "    def load_tavily_search(self, max_results: int = 2) -> TavilySearchResults:\n",
    "        return TavilySearchResults(max_results = max_results)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a44af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM ready: ChatGoogleGenerativeAI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(profile={}, client=<openai.resources.chat.completions.completions.Completions object at 0x000001D63BA8B810>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001D63CF4C890>, root_client=<openai.OpenAI object at 0x000001D63BA26BD0>, root_async_client=<openai.AsyncOpenAI object at 0x000001D63CF4C4D0>, model_name='google/gemma-3-4b', temperature=0.5, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='http://localhost:1234/v1')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings = LLMSettings()\n",
    "llm = settings.load_local(model_id=0)\n",
    "llm2 = settings.load_gemini()\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72d57ec",
   "metadata": {},
   "source": [
    "### import all necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec25e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# --- CHANGED SECTION ---\n",
    "# In v1.2.0, these moved to langchain_classic\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "# -----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb1bf7b",
   "metadata": {},
   "source": [
    "### Load and Split the Markdown Files\n",
    "This step reads your .md files from the food folder and cuts them into small \"chunks\" (1000 characters long) so the AI can process them easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e806c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 document(s).\n",
      "Created 79 chunks from the documents.\n"
     ]
    }
   ],
   "source": [
    "# Load all markdown files from the 'food' folder\n",
    "# We use \"**/*.md\" to find files even if they are in subfolders\n",
    "loader = DirectoryLoader('./food', glob=\"**/*.md\", loader_cls=UnstructuredMarkdownLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(docs)} document(s).\")\n",
    "\n",
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Created {len(splits)} chunks from the documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daae80b",
   "metadata": {},
   "source": [
    "### Initialize Local Embeddings & Vector Store\n",
    "We will use HuggingFaceEmbeddings to convert your text into numbers. This runs entirely on your CPU/GPU locally and is often faster than sending text to LM Studio for embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eeb8a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\langchain_tutorial\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\amalv\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# 3. Create Embeddings\n",
    "# \"all-MiniLM-L6-v2\" is a small, fast, and effective model for English text\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 4. Create Vector Store\n",
    "# This creates a local database in memory (or persists it if you add a persist_directory)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_model)\n",
    "\n",
    "# Turn the vectorstore into a retriever that the chain can use\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6160385d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data structure defined.\n"
     ]
    }
   ],
   "source": [
    "# --- CHANGED IMPORTS ---\n",
    "from pydantic import BaseModel, Field  # Import directly from Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser, PydanticOutputParser\n",
    "# -----------------------\n",
    "\n",
    "from typing import List\n",
    "\n",
    "# 1. Define the desired structure\n",
    "class FoodData(BaseModel):\n",
    "    food: str = Field(description=\"The name of the food item\")\n",
    "    info: str = Field(description=\"A brief description of the food item\")\n",
    "    background: str = Field(description=\"A brief explanation of background of the food item\")\n",
    "    ingredients: List[str] = Field(description=\"A list of ingredients used, mentioned in document\")\n",
    "    preparation: List[str] = Field(description=\"Step-by-step preparation process given in the document\")\n",
    "\n",
    "# 2. Create the parser\n",
    "parser = JsonOutputParser(pydantic_object=FoodData)\n",
    "\n",
    "print(\"Data structure defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0886e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Get formatting instructions for the LLM\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "# 4. Update the System Prompt to include these instructions\n",
    "# Notice we added \"\\n{format_instructions}\" at the end\n",
    "system_prompt = (\n",
    "    \"You are a culinary assistant. \"\n",
    "    \"Answer the user's question based ONLY on the context provided. \"\n",
    "    \"If you cannot find any context, add some\"\n",
    "    \"\\n\\n\"\n",
    "    \"{format_instructions}\"\n",
    "    \"\\n\\n\"\n",
    "    \"Context:\\n{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# 5. Re-create the chain\n",
    "# Note: We inject the partial_variables so the parser instructions are always there\n",
    "prompt = prompt.partial(format_instructions=format_instructions)\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c04ddcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Chain constructed successfully.\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the System Prompt\n",
    "system_prompt = (\n",
    "    \"You are a helpful culinary assistant. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question. \"\n",
    "    \"If the answer is not in the context, say you don't know.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# 2. Create the Prompt Template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# 3. Create the Processing Chain (LLM + Prompt)\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# 4. Create the Retrieval Chain (Retriever + Processing Chain)\n",
    "rag_chain2 = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "print(\"RAG Chain constructed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6e683b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking: Tell me about the brief information and historical background on sushi. Explain the ingredients and step by step preparation details....\n",
      "\n",
      "--- AI ANSWER ---\n",
      "```json\n",
      "{\"food\": \"Sushi\", \"info\": \"Sushi is a traditional Japanese dish centered on vinegared rice combined with seafood, vegetables, and occasionally tropical fruits. Contrary to common misconception, sushi is defined by its seasoned rice rather than raw fish. Sushi represents precision, balance, and respect for ingredients, making it one of the most technically refined cuisines in the world.\", \"background\": \"The origins of sushi trace back to ancient preservation methods in Southeast Asia, where fish was fermented with rice to extend shelf life. This method evolved in Japan into narezushi, and later into a fast food style featuring fresh fish and vinegared rice, sold by street vendors in Tokyo. Over time, sushi spread globally, adapting to local tastes while influencing global fine dining standards.\", \"ingredients\": [\"Short-grain Japanese rice\", \"Rice vinegar\", \"Sugar\", \"Salt\"], \"preparation\": [\"\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "food = \"sushi\"\n",
    "query = f\"Tell me about the brief information and historical background on {food}. Explain the ingredients and step by step preparation details.\"\n",
    "\n",
    "print(f\"Asking: {query}...\\n\")\n",
    "\n",
    "# Run the chain\n",
    "response = rag_chain.invoke({\"input\": query})\n",
    "\n",
    "print(\"--- AI ANSWER ---\")\n",
    "print(response[\"answer\"])\n",
    "\n",
    "# Optional: See which file it pulled the info from\n",
    "# print(\"\\n--- SOURCES ---\")\n",
    "# for doc in response[\"context\"]:\n",
    "#     print(f\"- {doc.metadata.get('source')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "542c906a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DISH: Sushi ---\n",
      "{'food': 'Sushi', 'info': 'Sushi is a traditional Japanese dish centered on vinegared rice combined with seafood, vegetables, and occasionally tropical fruits. Contrary to common misconception, sushi is defined by its seasoned rice rather than raw fish. Sushi represents precision, balance, and respect for ingredients, making it one of the most technically refined cuisines in the world.', 'background': 'The origins of sushi trace back to ancient preservation methods in Southeast Asia, where fish was fermented with rice to extend shelf life. This method evolved in Japan into narezushi, and later into a fast food style featuring fresh fish and vinegared rice, sold by street vendors in Tokyo. Over time, sushi spread globally, adapting to local tastes while influencing global fine dining standards.', 'ingredients': ['Short-grain Japanese rice', 'Rice vinegar', 'Sugar', 'Salt'], 'preparation': ['']}\n"
     ]
    }
   ],
   "source": [
    "raw_answer = response[\"answer\"]\n",
    "\n",
    "# --- NEW FIX STARTS HERE ---\n",
    "# Local models often wrap JSON in markdown blocks. We strip them out.\n",
    "if \"```\" in raw_answer:\n",
    "    # Remove ```json and ``` at the end\n",
    "    cleaned_answer = raw_answer.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "else:\n",
    "    cleaned_answer = raw_answer\n",
    "\n",
    "# Now parse the CLEANED string\n",
    "try:\n",
    "    data = parser.parse(cleaned_answer)\n",
    "    \n",
    "    print(f\"--- DISH: {data['food']} ---\")\n",
    "    print(data)\n",
    "\n",
    "    # ... (Your YouTube code here)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error parsing. Here is what the model sent:\")\n",
    "    print(cleaned_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d0649d",
   "metadata": {},
   "source": [
    "### Creating a chain to get youtube link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86451200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import YouTubeSearchTool\n",
    "\n",
    "# Initialize the tool\n",
    "youtube_tool = YouTubeSearchTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99191d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'food': 'Sushi',\n",
       " 'info': 'Sushi is a traditional Japanese dish centered on vinegared rice combined with seafood, vegetables, and occasionally tropical fruits. Contrary to common misconception, sushi is defined by its seasoned rice rather than raw fish. Sushi represents precision, balance, and respect for ingredients, making it one of the most technically refined cuisines in the world.',\n",
       " 'background': 'The origins of sushi trace back to ancient preservation methods in Southeast Asia, where fish was fermented with rice to extend shelf life. This method evolved in Japan into narezushi, and later into a fast food style featuring fresh fish and vinegared rice, sold by street vendors in Tokyo. Over time, sushi spread globally, adapting to local tastes while influencing global fine dining standards.',\n",
       " 'ingredients': ['Short-grain Japanese rice', 'Rice vinegar', 'Sugar', 'Salt'],\n",
       " 'preparation': ['']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = parser.parse(response[\"answer\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "523442bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How to cook Sushi in detail'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_search_query = f\"How to cook {data['food']} in detail\"\n",
    "\n",
    "youtube_search_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51fc7387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Video:\n",
      "['https://www.youtube.com/watch?v=nIoOv6lWYnk&pp=ygUbSG93IHRvIGNvb2sgU3VzaGkgaW4gZGV0YWls', 'https://www.youtube.com/watch?v=ovX3X3vjywI&pp=ygUbSG93IHRvIGNvb2sgU3VzaGkgaW4gZGV0YWls']\n"
     ]
    }
   ],
   "source": [
    "video_links = youtube_tool.run(youtube_search_query)\n",
    "    \n",
    "# Display the result\n",
    "print(f\"Recommended Video:\\n{video_links}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
