{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c618c953",
   "metadata": {},
   "source": [
    "# 4. Tools and Agents\n",
    "\n",
    "## Syllabus\n",
    "* 4.1 Introduction to Tools\n",
    "* 4.2 Introduction to Agents\n",
    "* 4.3 Built-in Tools in LangChain\n",
    "* 4.4 Agents in Action\n",
    "* 4.5 Building a Multi-Tool Agent\n",
    "* 4.6 Creating Custom Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb601f33",
   "metadata": {},
   "source": [
    "### 4.1 Introduction to Tools\n",
    "A tool in LangChain is simply a function that LLM uses when it cannot do on its own.\n",
    "eg. A calculator is a tool that an LLM can use because LLM's can make math mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a900558",
   "metadata": {},
   "source": [
    "Every tool has 3 main parts\n",
    "* Name: Unique identifier used by the agent\n",
    "* Description: Natural language instruction so the agent knows when to use it\n",
    "* Function: Actual python function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9625216f",
   "metadata": {},
   "source": [
    "## 4.2 Built-in Tools in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef1d81",
   "metadata": {},
   "source": [
    "#### Tool Example 1: Calculator Tool\n",
    "\n",
    "* `PythonREPLTool()` gives the LLM access to a Python REPL(read, eval, print, loop)\n",
    "* It executes the given string as a python code\n",
    "* creates a small sandbox python interpretor\n",
    "* does not require an LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d9c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c5c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the llm\n",
    "from llm.load_llm import initialize_llm\n",
    "\n",
    "llm = initialize_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d2841a",
   "metadata": {},
   "source": [
    "`pip install langchain-experimental`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "calculator = PythonREPLTool()\n",
    "\n",
    "# Add print() to see the results\n",
    "print(calculator.run(\"print(5*12+(33/3))\"))\n",
    "print(calculator.run(\"print(max(25,69))\"))\n",
    "print(calculator.run(\"print(sum([i for i in range(1,11)]))\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7901628f",
   "metadata": {},
   "source": [
    "`LLMMathChain`\n",
    "* Instead of directly executing code, it asks the LLM to write math as python code, then evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3767d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "math_chain = LLMMathChain.from_llm(llm = llm, verbose = True)\n",
    "print(math_chain.run(\"What is (5*12+(33/3))\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098071ad",
   "metadata": {},
   "source": [
    "#### Tool Example 2: Search Tool\n",
    "* LLM's don't have access to fresh information like daily news, stock prices, weather etc\n",
    "* To fix this, we connect them to a search API.\n",
    "\n",
    "* There are a few tools for searching\n",
    "    - Tavily Search API\n",
    "    - SerpAPI\n",
    "    - DuckDuckGoSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e8eafa",
   "metadata": {},
   "source": [
    "`pip install tavily-python`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ba0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "tavily_key = os.environ.get(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525e5b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults(max_results = 2)\n",
    "\n",
    "results = search.run(\"What is the latest weather in Kochi, Kerala, India\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e766dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[0][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c104f",
   "metadata": {},
   "source": [
    "* Alternatively we can use `DuckDuckGo search` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11653607",
   "metadata": {},
   "source": [
    "### Using Search in an Agent\n",
    "üìå Here‚Äôs what happens:\n",
    "- LLM sees the question.\n",
    "- Decides ‚Üí ‚ÄúI don‚Äôt know Bitcoin‚Äôs price, I must use search.‚Äù\n",
    "- Calls search tool ‚Üí fetches live price.\n",
    "- Passes that result into the calculator.\n",
    "- Returns final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695ddd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "# Give the agent tools (search + calculator)\n",
    "tools = [search, calculator]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Now the LLM can decide when to search\n",
    "agent.run(\"What is the current price of Bitcoin in USD divided by 3?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7523fc9f",
   "metadata": {},
   "source": [
    "## 4.3 Creating Custom Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ef23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "# define a custom tool with a decorator function\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\" Get the weather for a given city from these dummy data\"\"\"\n",
    "    weather_data = {\n",
    "        \"New York\": \"Sunny, 25¬∞C\",\n",
    "        \"London\": \"Rainy, 15¬∞C\",\n",
    "        \"Tokyo\": \"Cloudy, 20¬∞C\"\n",
    "    }\n",
    "\n",
    "    return weather_data[city]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cdf672",
   "metadata": {},
   "source": [
    "using it in an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daaa275",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_weather]\n",
    "\n",
    "agent2 = initialize_agent(\n",
    "    tools = tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose= True\n",
    ")\n",
    "\n",
    "agent2.run(\"Whats the weather in London\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0393417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def matrix_determinant(matrix: str)->str:\n",
    "    \"\"\"\n",
    "    Calculates the determinant of a 2x2 matrix\n",
    "    Input: 2x2 matrix in string format\n",
    "    Output: determinant value in string\n",
    "    \"\"\"\n",
    "    arr = np.array(eval(matrix))\n",
    "    det = np.linalg.det(arr)\n",
    "    return f\"The determinant is {det}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bccd295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "tools3 = [matrix_determinant]\n",
    "agent3 = initialize_agent(\n",
    "    llm = llm,\n",
    "    tools = tools3,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "agent3.run(\"Determine the determinant of matrix [[1,2],[3,4]]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942c3231",
   "metadata": {},
   "source": [
    "## 4.4 Introduction to Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce3332d",
   "metadata": {},
   "source": [
    "An agent is like an AI decision maker that\n",
    "* takes in a user query as input\n",
    "* Decides how to solve it (tool or reasoning)\n",
    "* uses the chosen tools\n",
    "* returns the final answer\n",
    "\n",
    "eg. What is the square root of population of Japan.\n",
    "* Pick at tool:\n",
    "    - Search tool to get population\n",
    "    - Calculator tool to get square root\n",
    "    - Database tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e046d5e8",
   "metadata": {},
   "source": [
    "#### ReAct Framework (Reason + Act)\n",
    "* The agent reasons step by step\n",
    "* Then it acts by using tools.\n",
    "\n",
    "eg. for previous example\n",
    "* Reason: I don't know the population of Japan?\n",
    "* Act: Use the search tool\n",
    "* Observation: 125M\n",
    "* Reason: I need to calculate the root of 125M\n",
    "* Act: use the calculator\n",
    "* Observation: result is 11180\n",
    "* Response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda1ea4",
   "metadata": {},
   "source": [
    "#### Agent Execution Loop\n",
    "\n",
    "This is the core cycle of how agents work:\n",
    "\n",
    "1. Thought ‚Üí The agent reasons internally. (\"I need Japan‚Äôs population.\")\n",
    "2. Action ‚Üí Calls a tool. (\"Search for Japan population.\")\n",
    "3. Observation ‚Üí Gets result from tool. (\"125M.\")\n",
    "4. Thought ‚Üí Decides next step. (\"Now calculate the square root.\")\n",
    "5. Action ‚Üí Uses calculator.\n",
    "6. Observation ‚Üí Gets the number.\n",
    "7. Response ‚Üí Returns final answer to user.\n",
    "\n",
    "üëâ This loop continues until the agent has enough info to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bb895d",
   "metadata": {},
   "source": [
    "## 4.5 Agents in Action\n",
    "\n",
    "#### Zero-Shot ReAct Description Agent\n",
    "\n",
    "* Zero-Shot ‚Üí means the agent hasn‚Äôt been given examples ahead of time.\n",
    "* ReAct ‚Üí means it uses the Reason + Act loop we discussed.\n",
    "* Description ‚Üí it decides which tool to use based only on the natural language description of tools.\n",
    "\n",
    "üëâ Example tools given to the agent:\n",
    "\n",
    "Calculator: \"Useful for math operations like addition, subtraction, multiplication, division, or square roots.\"\n",
    "\n",
    "Search: \"Useful for finding real-world facts from the internet.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041f7291",
   "metadata": {},
   "source": [
    "* Lets build a small agent that takes in names of some countries\n",
    "* Finds their population\n",
    "* Square roots them and store it in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ddb150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "import math\n",
    "import csv\n",
    "# Calculator Tool\n",
    "@tool\n",
    "def square_root_tool(population: str) -> str:\n",
    "    \"\"\"\n",
    "    This tool takes in a population as string and returns its square root in string format\n",
    "    \"\"\"\n",
    "    value = int(population)\n",
    "    return str(math.sqrt(value))\n",
    "\n",
    "@tool\n",
    "def write_to_csv(row: str) -> None:\n",
    "    \"\"\"\n",
    "    This tool takes a row as string and writes it into a csv file.\n",
    "    The input must be of the form city,population,square_root_of_population.\n",
    "    eg: \"Tokyo 125000000 11180\"\n",
    "    \"\"\"\n",
    "    item = row.split(\" \")\n",
    "    file = open(\"output.csv\" , \"a+\")\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(item)\n",
    "    file.close()\n",
    "\n",
    "@tool\n",
    "def read_from_csv(dummy: str = \"read\") -> str:\n",
    "    \"\"\"\n",
    "    This tool reads rows from the csv file and returns them as a string.\n",
    "    Input is ignored (just pass anything).\n",
    "    \"\"\"\n",
    "    with open(\"output.csv\", \"r\") as file:\n",
    "        data = file.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa007e3c",
   "metadata": {},
   "source": [
    "creating an agent that will take a couple of cities, find its population, find it square root and save it into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9161343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent4 = initialize_agent(\n",
    "    tools = [search, square_root_tool , write_to_csv, read_from_csv],\n",
    "    llm = llm,\n",
    "    agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "cities = \"London, Tokyo, New York\"\n",
    "agent4.run(f\"You are an agent that will fetch population count of these cities {cities}. After finding the population, find the square root of each city and write it into the csv file in the format city,population,root. Display the final csv file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fee588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "\n",
    "agent5 = initialize_agent(\n",
    "    tools = [search],\n",
    "    llm=llm2,\n",
    "    agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True,\n",
    "    handle_parsing_error = True\n",
    ")\n",
    "\n",
    "agent5.invoke({\"input\":\"Who is the current president of the United States\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33783936",
   "metadata": {},
   "source": [
    "performing with LM studio model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92baba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm3 = ChatOpenAI(\n",
    "    model=\"google/gemma-3-4b\",\n",
    "    openai_api_key = 'lm-studio', # type: ignore\n",
    "    openai_api_base=\"http://localhost:1234/v1\" # type: ignore\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d42e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm3.invoke(\"Who is the current president of USA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9206c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "react_prompt = \"\"\"You are a helpful assistant that uses tools.\n",
    "When answering, you must strictly follow this format:\n",
    "\n",
    "Question: the input question to answer\n",
    "Thought: your reasoning about what to do\n",
    "Action: the exact name of the tool to use, nothing else\n",
    "Action Input: the input to the tool, in plain text\n",
    "\n",
    "If you already have the answer, respond like this:\n",
    "\n",
    "Thought: I know the answer\n",
    "Final Answer: <the answer>\n",
    "\n",
    "Never add explanations after Action or Action Input.\n",
    "\"\"\"\n",
    "\n",
    "agent6 = initialize_agent(\n",
    "    tools = [search],\n",
    "    llm=llm3,\n",
    "    agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True,\n",
    "    handle_parsing_error = True, \n",
    ")\n",
    "\n",
    "agent6.invoke({\"input\":\"Who is the current president of the United States\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87b8881",
   "metadata": {},
   "source": [
    "## 4.6 Agent Types\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# üß† LangChain Agent Types\n",
    "\n",
    "LangChain provides different **agent types**, each with its own reasoning style, memory handling, and tool support. Agents differ in **how they decide when and how to use tools**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Zero-Shot ReAct Description\n",
    "\n",
    "**Description**\n",
    "\n",
    "* Uses the **ReAct (Reason + Act)** framework.\n",
    "* LLM decides which tool to call based only on tool descriptions.\n",
    "* No prior examples (‚Äúzero-shot‚Äù).\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "* General-purpose agents.\n",
    "* Tasks where the model needs to reason step by step and use tools.\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "* Supports only **single-input tools** (`str -> str`).\n",
    "* No memory between calls.\n",
    "\n",
    "**Example**\n",
    "\n",
    "```python\n",
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Conversational ReAct Description\n",
    "\n",
    "**Description**\n",
    "\n",
    "* Same as Zero-Shot ReAct, but includes **memory** of the conversation.\n",
    "* Lets you ask follow-up questions without repeating context.\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "* Chatbots and assistants that need dialogue memory.\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "* Still only works with **single-input tools**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Self-Ask with Search\n",
    "\n",
    "**Description**\n",
    "\n",
    "* Agent decomposes a query into **sub-questions**.\n",
    "* Uses **search tools** when it cannot answer from reasoning alone.\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "* Knowledge Q\\&A.\n",
    "* Tasks where an LLM must fetch external information.\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "* Not general-purpose ‚Äî designed specifically for search-based reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Plan-and-Execute\n",
    "\n",
    "**Description**\n",
    "\n",
    "* Works in **two stages**:\n",
    "\n",
    "  1. LLM **plans** the full set of steps.\n",
    "  2. Then **executes** each step with sub-agents.\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "* Complex, multi-step workflows.\n",
    "* Scenarios requiring structured execution instead of ad-hoc reasoning.\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "* Can be slower (planning + execution).\n",
    "* Sometimes ‚Äúover-plans.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Structured Chat Agent\n",
    "\n",
    "**Description**\n",
    "\n",
    "* Unlike Zero-Shot, it supports **multi-input tools**.\n",
    "* LLM calls tools with structured arguments (like a JSON dict).\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "* When tools need multiple parameters (e.g., `{city: \"Tokyo\", population: 125000000}`).\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "* Requires stricter prompts and well-defined tool schemas.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Functions Agent (OpenAI/Gemini)\n",
    "\n",
    "**Description**\n",
    "\n",
    "* Uses **function-calling APIs** of LLMs (OpenAI, Gemini, etc.).\n",
    "* LLM outputs valid JSON with function arguments.\n",
    "* Very reliable for tool use.\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "* If your LLM supports function calling (e.g., GPT-4, Gemini).\n",
    "* Production-grade tool invocation.\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "* Dependent on LLM providers that implement function calling.\n",
    "\n",
    "---\n",
    "\n",
    "# üìä Summary Table\n",
    "\n",
    "| Agent Type                      | Multi-Input Tools? | Memory? | Best For                  |\n",
    "| ------------------------------- | ------------------ | ------- | ------------------------- |\n",
    "| Zero-Shot ReAct Description     | ‚ùå No               | ‚ùå No    | General tool use          |\n",
    "| Conversational ReAct            | ‚ùå No               | ‚úÖ Yes   | Chat assistants           |\n",
    "| Self-Ask with Search            | ‚ùå No               | ‚ùå No    | Knowledge Q\\&A            |\n",
    "| Plan-and-Execute                | ‚ùå No               | ‚ùå No    | Complex workflows         |\n",
    "| Structured Chat Agent           | ‚úÖ Yes              | ‚ùå No    | Multi-input tools         |\n",
    "| Functions Agent (OpenAI/Gemini) | ‚úÖ Yes              | ‚ùå No    | Reliable structured calls |\n",
    "\n",
    "---\n",
    "\n",
    "‚ö° **Tip**: Start with **Zero-Shot ReAct** (simplest), then move to **Structured Chat** or **Functions Agent** once your tools get more complex.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
