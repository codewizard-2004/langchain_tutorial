{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c618c953",
   "metadata": {},
   "source": [
    "# 4. Tools and Agents\n",
    "\n",
    "## Syllabus\n",
    "* 4.1 Introduction to Tools\n",
    "* 4.2 Introduction to Agents\n",
    "* 4.3 Built-in Tools in LangChain\n",
    "* 4.4 Agents in Action\n",
    "* 4.5 Building a Multi-Tool Agent\n",
    "* 4.6 Creating Custom Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb601f33",
   "metadata": {},
   "source": [
    "### 4.1 Introduction to Tools\n",
    "A tool in LangChain is simply a function that LLM uses when it cannot do on its own.\n",
    "eg. A calculator is a tool that an LLM can use because LLM's can make math mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a900558",
   "metadata": {},
   "source": [
    "Every tool has 3 main parts\n",
    "* Name: Unique identifier used by the agent\n",
    "* Description: Natural language instruction so the agent knows when to use it\n",
    "* Function: Actual python function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9625216f",
   "metadata": {},
   "source": [
    "## 4.2 Built-in Tools in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef1d81",
   "metadata": {},
   "source": [
    "#### Tool Example 1: Calculator Tool\n",
    "\n",
    "* `PythonREPLTool()` gives the LLM access to a Python REPL(read, eval, print, loop)\n",
    "* It executes the given string as a python code\n",
    "* creates a small sandbox python interpretor\n",
    "* does not require an LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d9c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c5c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the llm\n",
    "from llm.load_llm import initialize_llm\n",
    "\n",
    "llm = initialize_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d2841a",
   "metadata": {},
   "source": [
    "`pip install langchain-experimental`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "calculator = PythonREPLTool()\n",
    "\n",
    "# Add print() to see the results\n",
    "print(calculator.run(\"print(5*12+(33/3))\"))\n",
    "print(calculator.run(\"print(max(25,69))\"))\n",
    "print(calculator.run(\"print(sum([i for i in range(1,11)]))\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7901628f",
   "metadata": {},
   "source": [
    "`LLMMathChain`\n",
    "* Instead of directly executing code, it asks the LLM to write math as python code, then evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3767d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "math_chain = LLMMathChain.from_llm(llm = llm, verbose = True)\n",
    "print(math_chain.run(\"What is (5*12+(33/3))\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098071ad",
   "metadata": {},
   "source": [
    "#### Tool Example 2: Search Tool\n",
    "* LLM's don't have access to fresh information like daily news, stock prices, weather etc\n",
    "* To fix this, we connect them to a search API.\n",
    "\n",
    "* There are a few tools for searching\n",
    "    - Tavily Search API\n",
    "    - SerpAPI\n",
    "    - DuckDuckGoSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e8eafa",
   "metadata": {},
   "source": [
    "`pip install tavily-python`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ba0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "tavily_key = os.environ.get(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525e5b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults(max_results = 2)\n",
    "\n",
    "results = search.run(\"What is the latest weather in Kochi, Kerala, India\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e766dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[0][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c104f",
   "metadata": {},
   "source": [
    "* Alternatively we can use `DuckDuckGo search` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11653607",
   "metadata": {},
   "source": [
    "### Using Search in an Agent\n",
    "📌 Here’s what happens:\n",
    "- LLM sees the question.\n",
    "- Decides → “I don’t know Bitcoin’s price, I must use search.”\n",
    "- Calls search tool → fetches live price.\n",
    "- Passes that result into the calculator.\n",
    "- Returns final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695ddd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "# Give the agent tools (search + calculator)\n",
    "tools = [search, calculator]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Now the LLM can decide when to search\n",
    "agent.run(\"What is the current price of Bitcoin in USD divided by 3?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7523fc9f",
   "metadata": {},
   "source": [
    "## 4.3 Creating Custom Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ef23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "# define a custom tool with a decorator function\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\" Get the weather for a given city from these dummy data\"\"\"\n",
    "    weather_data = {\n",
    "        \"New York\": \"Sunny, 25°C\",\n",
    "        \"London\": \"Rainy, 15°C\",\n",
    "        \"Tokyo\": \"Cloudy, 20°C\"\n",
    "    }\n",
    "\n",
    "    return weather_data[city]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cdf672",
   "metadata": {},
   "source": [
    "using it in an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daaa275",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_weather]\n",
    "\n",
    "agent2 = initialize_agent(\n",
    "    tools = tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose= True\n",
    ")\n",
    "\n",
    "agent2.run(\"Whats the weather in London\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0393417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def matrix_determinant(matrix: str)->str:\n",
    "    \"\"\"\n",
    "    Calculates the determinant of a 2x2 matrix\n",
    "    Input: 2x2 matrix in string format\n",
    "    Output: determinant value in string\n",
    "    \"\"\"\n",
    "    arr = np.array(eval(matrix))\n",
    "    det = np.linalg.det(arr)\n",
    "    return f\"The determinant is {det}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bccd295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "tools3 = [matrix_determinant]\n",
    "agent3 = initialize_agent(\n",
    "    llm = llm,\n",
    "    tools = tools3,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "agent3.run(\"Determine the determinant of matrix [[1,2],[3,4]]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942c3231",
   "metadata": {},
   "source": [
    "## 4.4 Introduction to Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce3332d",
   "metadata": {},
   "source": [
    "An agent is like an AI decision maker that\n",
    "* takes in a user query as input\n",
    "* Decides how to solve it (tool or reasoning)\n",
    "* uses the chosen tools\n",
    "* returns the final answer\n",
    "\n",
    "eg. What is the square root of population of Japan.\n",
    "* Pick at tool:\n",
    "    - Search tool to get population\n",
    "    - Calculator tool to get square root\n",
    "    - Database tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e046d5e8",
   "metadata": {},
   "source": [
    "#### ReAct Framework (Reason + Act)\n",
    "* The agent reasons step by step\n",
    "* Then it acts by using tools.\n",
    "\n",
    "eg. for previous example\n",
    "* Reason: I don't know the population of Japan?\n",
    "* Act: Use the search tool\n",
    "* Observation: 125M\n",
    "* Reason: I need to calculate the root of 125M\n",
    "* Act: use the calculator\n",
    "* Observation: result is 11180\n",
    "* Response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda1ea4",
   "metadata": {},
   "source": [
    "#### Agent Execution Loop\n",
    "\n",
    "This is the core cycle of how agents work:\n",
    "\n",
    "1. Thought → The agent reasons internally. (\"I need Japan’s population.\")\n",
    "2. Action → Calls a tool. (\"Search for Japan population.\")\n",
    "3. Observation → Gets result from tool. (\"125M.\")\n",
    "4. Thought → Decides next step. (\"Now calculate the square root.\")\n",
    "5. Action → Uses calculator.\n",
    "6. Observation → Gets the number.\n",
    "7. Response → Returns final answer to user.\n",
    "\n",
    "👉 This loop continues until the agent has enough info to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bb895d",
   "metadata": {},
   "source": [
    "## 4.5 Agents in Action\n",
    "\n",
    "#### Zero-Shot ReAct Description Agent\n",
    "\n",
    "* Zero-Shot → means the agent hasn’t been given examples ahead of time.\n",
    "* ReAct → means it uses the Reason + Act loop we discussed.\n",
    "* Description → it decides which tool to use based only on the natural language description of tools.\n",
    "\n",
    "👉 Example tools given to the agent:\n",
    "\n",
    "Calculator: \"Useful for math operations like addition, subtraction, multiplication, division, or square roots.\"\n",
    "\n",
    "Search: \"Useful for finding real-world facts from the internet.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041f7291",
   "metadata": {},
   "source": [
    "* Lets build a small agent that takes in names of some countries\n",
    "* Finds their population\n",
    "* Square roots them and store it in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ddb150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "import math\n",
    "import csv\n",
    "# Calculator Tool\n",
    "@tool\n",
    "def square_root_tool(population: str) -> str:\n",
    "    \"\"\"\n",
    "    This tool takes in a population as string and returns its square root in string format\n",
    "    \"\"\"\n",
    "    value = int(population)\n",
    "    return str(math.sqrt(value))\n",
    "\n",
    "@tool\n",
    "def write_to_csv(row: str) -> None:\n",
    "    \"\"\"\n",
    "    This tool takes a row as string and writes it into a csv file.\n",
    "    The input must be of the form city,population,square_root_of_population.\n",
    "    eg: \"Tokyo 125000000 11180\"\n",
    "    \"\"\"\n",
    "    item = row.split(\" \")\n",
    "    file = open(\"output.csv\" , \"a+\")\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(item)\n",
    "    file.close()\n",
    "\n",
    "@tool\n",
    "def read_from_csv(dummy: str = \"read\") -> str:\n",
    "    \"\"\"\n",
    "    This tool reads rows from the csv file and returns them as a string.\n",
    "    Input is ignored (just pass anything).\n",
    "    \"\"\"\n",
    "    with open(\"output.csv\", \"r\") as file:\n",
    "        data = file.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa007e3c",
   "metadata": {},
   "source": [
    "creating an agent that will take a couple of cities, find its population, find it square root and save it into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9161343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent4 = initialize_agent(\n",
    "    tools = [search, square_root_tool , write_to_csv, read_from_csv],\n",
    "    llm = llm,\n",
    "    agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "cities = \"London, Tokyo, New York\"\n",
    "agent4.run(f\"You are an agent that will fetch population count of these cities {cities}. After finding the population, find the square root of each city and write it into the csv file in the format city,population,root. Display the final csv file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fee588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "\n",
    "agent5 = initialize_agent(\n",
    "    tools = [search],\n",
    "    llm=llm2,\n",
    "    agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True,\n",
    "    handle_parsing_error = True\n",
    ")\n",
    "\n",
    "agent5.invoke({\"input\":\"Who is the current president of the United States\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33783936",
   "metadata": {},
   "source": [
    "performing with LM studio model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92baba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm3 = ChatOpenAI(\n",
    "    model=\"google/gemma-3-4b\",\n",
    "    openai_api_key = 'lm-studio', # type: ignore\n",
    "    openai_api_base=\"http://localhost:1234/v1\" # type: ignore\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d42e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm3.invoke(\"Who is the current president of USA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9206c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "react_prompt = \"\"\"You are a helpful assistant that uses tools.\n",
    "When answering, you must strictly follow this format:\n",
    "\n",
    "Question: the input question to answer\n",
    "Thought: your reasoning about what to do\n",
    "Action: the exact name of the tool to use, nothing else\n",
    "Action Input: the input to the tool, in plain text\n",
    "\n",
    "If you already have the answer, respond like this:\n",
    "\n",
    "Thought: I know the answer\n",
    "Final Answer: <the answer>\n",
    "\n",
    "Never add explanations after Action or Action Input.\n",
    "\"\"\"\n",
    "\n",
    "agent6 = initialize_agent(\n",
    "    tools = [search],\n",
    "    llm=llm3,\n",
    "    agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True,\n",
    "    handle_parsing_error = True, \n",
    ")\n",
    "\n",
    "agent6.invoke({\"input\":\"Who is the current president of the United States\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87b8881",
   "metadata": {},
   "source": [
    "## 4.6 Agent Types\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# 🧠 LangChain Agent Types\n",
    "\n",
    "LangChain provides different **agent types**, each with its own reasoning style, memory handling, and tool support. Agents differ in **how they decide when and how to use tools**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Zero-Shot ReAct Description\n",
    "\n",
    "**Description**\n",
    "\n",
    "* Uses the **ReAct (Reason + Act)** framework.\n",
    "* LLM decides which tool to call based only on tool descriptions.\n",
    "* No prior examples (“zero-shot”).\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "* General-purpose agents.\n",
    "* Tasks where the model needs to reason step by step and use tools.\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "* Supports only **single-input tools** (`str -> str`).\n",
    "* No memory between calls.\n",
    "\n",
    "**Example**\n",
    "\n",
    "```python\n",
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Conversational ReAct Description\n",
    "\n",
    "**Description**\n",
    "\n",
    "* Same as Zero-Shot ReAct, but includes **memory** of the conversation.\n",
    "* Lets you ask follow-up questions without repeating context.\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "* Chatbots and assistants that need dialogue memory.\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "* Still only works with **single-input tools**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Self-Ask with Search\n",
    "\n",
    "**Description**\n",
    "\n",
    "* Agent decomposes a query into **sub-questions**.\n",
    "* Uses **search tools** when it cannot answer from reasoning alone.\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "* Knowledge Q\\&A.\n",
    "* Tasks where an LLM must fetch external information.\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "* Not general-purpose — designed specifically for search-based reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Plan-and-Execute\n",
    "\n",
    "**Description**\n",
    "\n",
    "* Works in **two stages**:\n",
    "\n",
    "  1. LLM **plans** the full set of steps.\n",
    "  2. Then **executes** each step with sub-agents.\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "* Complex, multi-step workflows.\n",
    "* Scenarios requiring structured execution instead of ad-hoc reasoning.\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "* Can be slower (planning + execution).\n",
    "* Sometimes “over-plans.”\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Structured Chat Agent\n",
    "\n",
    "**Description**\n",
    "\n",
    "* Unlike Zero-Shot, it supports **multi-input tools**.\n",
    "* LLM calls tools with structured arguments (like a JSON dict).\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "* When tools need multiple parameters (e.g., `{city: \"Tokyo\", population: 125000000}`).\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "* Requires stricter prompts and well-defined tool schemas.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Functions Agent (OpenAI/Gemini)\n",
    "\n",
    "**Description**\n",
    "\n",
    "* Uses **function-calling APIs** of LLMs (OpenAI, Gemini, etc.).\n",
    "* LLM outputs valid JSON with function arguments.\n",
    "* Very reliable for tool use.\n",
    "\n",
    "**When to Use**\n",
    "\n",
    "* If your LLM supports function calling (e.g., GPT-4, Gemini).\n",
    "* Production-grade tool invocation.\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "* Dependent on LLM providers that implement function calling.\n",
    "\n",
    "---\n",
    "\n",
    "# 📊 Summary Table\n",
    "\n",
    "| Agent Type                      | Multi-Input Tools? | Memory? | Best For                  |\n",
    "| ------------------------------- | ------------------ | ------- | ------------------------- |\n",
    "| Zero-Shot ReAct Description     | ❌ No               | ❌ No    | General tool use          |\n",
    "| Conversational ReAct            | ❌ No               | ✅ Yes   | Chat assistants           |\n",
    "| Self-Ask with Search            | ❌ No               | ❌ No    | Knowledge Q\\&A            |\n",
    "| Plan-and-Execute                | ❌ No               | ❌ No    | Complex workflows         |\n",
    "| Structured Chat Agent           | ✅ Yes              | ❌ No    | Multi-input tools         |\n",
    "| Functions Agent (OpenAI/Gemini) | ✅ Yes              | ❌ No    | Reliable structured calls |\n",
    "\n",
    "---\n",
    "\n",
    "⚡ **Tip**: Start with **Zero-Shot ReAct** (simplest), then move to **Structured Chat** or **Functions Agent** once your tools get more complex.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
