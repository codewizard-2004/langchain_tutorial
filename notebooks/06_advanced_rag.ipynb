{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f6dc12",
   "metadata": {},
   "source": [
    "# Chapter 06 ‚Äì Advanced RAG Systems\n",
    "\n",
    "In this notebook we will:\n",
    "1. Use **chunking strategies** for better document ingestion.\n",
    "2. Improve **retrieval with re-ranking & compression**.\n",
    "3. Design **advanced prompts with citations**.\n",
    "4. Evaluate RAG answers with LangChain evaluators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "029d902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52250868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM ready: ChatGoogleGenerativeAI\n"
     ]
    }
   ],
   "source": [
    "# initializing the llm\n",
    "from llm.load_llm import initialize_llm\n",
    "\n",
    "llm = initialize_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa76910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "embeddings  = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f6b83",
   "metadata": {},
   "source": [
    "## 6.1 Load Documents and Apply Chunking\n",
    "\n",
    "We will use a text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf417478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"rag.txt\"))  # Should return True if the file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2902716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original docs: 1 | After chunking: 16\n",
      "## An Introduction to Modern AI Application Development\n",
      "\n",
      "The world of Large Language Models (LLMs) is rapidly evolving beyond simple chatbots. Developers are now building sophisticated applications that can reason, access external data, and perform complex tasks. Three fundamental concepts at the he\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"rag.txt\" ,  encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Chunking \n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunked_docs = splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Original docs: {len(docs)} | After chunking: {len(chunked_docs)}\")\n",
    "print(chunked_docs[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b006df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'rag.txt'}, page_content='## An Introduction to Modern AI Application Development\\n\\nThe world of Large Language Models (LLMs) is rapidly evolving beyond simple chatbots. Developers are now building sophisticated applications that can reason, access external data, and perform complex tasks. Three fundamental concepts at the heart of this revolution are **LangChain**, **Embeddings**, and **Retrieval-Augmented Generation (RAG)**. Understanding how these technologies work together is key to unlocking the full potential of AI.'),\n",
       " Document(metadata={'source': 'rag.txt'}, page_content='---\\n\\n### Embeddings: The Language of Machines üß†\\n\\nAt its core, a language model doesn\\'t understand words like \"cat\" or \"dog\" in the way humans do. It understands numbers. **Embeddings** are the bridge that translates our qualitative, nuanced language into the quantitative, numerical language of machines.'),\n",
       " Document(metadata={'source': 'rag.txt'}, page_content='An embedding is a **vector**‚Äîa list of numbers‚Äîthat represents a piece of text (a word, sentence, or entire document) in a high-dimensional space. The magic of embeddings is that they capture the *semantic meaning* or the \"vibe\" of the text.'),\n",
       " Document(metadata={'source': 'rag.txt'}, page_content='* **How it Works**: Words with similar meanings are located close to each other in this vector space. For example, the vectors for \"king\" and \"queen\" would be closer to each other than the vectors for \"king\" and \"banana.\"\\n* **Mathematical Relationships**: This spatial relationship allows for fascinating mathematical operations. The famous example is that the vector operation `vector(\\'king\\') - vector(\\'man\\') + vector(\\'woman\\')` results in a vector that is very close to `vector(\\'queen\\')`.'),\n",
       " Document(metadata={'source': 'rag.txt'}, page_content='This process allows an AI to grasp context, nuance, and relationships in text, which is fundamental for any advanced language task. Think of it like a librarian organizing books in a vast, multi-dimensional library. Instead of using the Dewey Decimal System, this librarian places books with similar topics and themes physically close to one another, making it easy to find related information.\\n\\n\\n\\n---\\n\\n### Retrieval-Augmented Generation (RAG): Giving LLMs a Brain Boost üìö'),\n",
       " Document(metadata={'source': 'rag.txt'}, page_content='One of the biggest limitations of standard LLMs is that their knowledge is frozen at the time of their training. They don\\'t know about current events and can sometimes \"hallucinate\" or make up facts. **Retrieval-Augmented Generation (RAG)** is an ingenious architecture designed to solve this problem.'),\n",
       " Document(metadata={'source': 'rag.txt'}, page_content=\"RAG connects an LLM to an external, up-to-date knowledge base, allowing it to provide answers that are grounded in specific, verifiable information. It's like giving a brilliant but forgetful professor an open-book exam with a curated library of reference materials.\\n\\nThe RAG process works in two main steps:\"),\n",
       " Document(metadata={'source': 'rag.txt'}, page_content='1.  **Retrieval**: When you ask a question (a \"query\"), the system first retrieves relevant information from your knowledge base (e.g., a collection of PDFs, a database, or a website). This is where embeddings are crucial. Your query is converted into an embedding, and the system searches for documents with the most similar embeddings in the knowledge base. This is called a **vector search**.'),\n",
       " Document(metadata={'source': 'rag.txt'}, page_content='2.  **Generation**: The retrieved documents are then passed to the LLM along with your original query. The LLM uses this provided context to formulate a comprehensive and accurate answer. The prompt essentially becomes: \"Using the following information [retrieved documents], answer this question [your query].\"'),\n",
       " Document(metadata={'source': 'rag.txt'}, page_content=\"This approach dramatically improves the accuracy of LLM responses, reduces hallucinations, and allows developers to build AI applications that can reason over private or domain-specific data.\\n\\n\\n\\n---\\n\\n### LangChain: The Developer's Toolkit for Building with LLMs üõ†Ô∏è\"),\n",
       " Document(metadata={'source': 'rag.txt'}, page_content='While embeddings and RAG are powerful concepts, implementing them from scratch can be complex. **LangChain** is an open-source framework designed to simplify the process of building applications powered by LLMs. It acts as a universal toolkit or a \"Swiss Army knife\" for AI developers.\\n\\nLangChain provides a standard set of components and \"chains\" that allow you to easily connect different parts of your application. Think of it like a set of LEGO bricks for building AI systems.'),\n",
       " Document(metadata={'source': 'rag.txt'}, page_content='Key components of LangChain include:'),\n",
       " Document(metadata={'source': 'rag.txt'}, page_content=\"* **Models**: Standardized interfaces for various LLMs (like OpenAI's GPT-4, Google's Gemini, etc.).\\n* **Prompts**: Tools for constructing and managing dynamic prompts that are sent to the LLM.\\n* **Indexes**: Components that help you structure and retrieve data from your knowledge bases. This is where you would manage the embeddings for your RAG system.\"),\n",
       " Document(metadata={'source': 'rag.txt'}, page_content='* **Chains**: The core of LangChain. Chains allow you to link different components together in a sequence. For example, you can create a chain that takes a user query, retrieves relevant documents from an index (the RAG step), formats a prompt with that information, and then sends it to the LLM for a final answer.'),\n",
       " Document(metadata={'source': 'rag.txt'}, page_content='* **Agents**: More advanced chains that use an LLM to decide which actions to take and in what order. An agent can be given access to tools (like a search engine, a calculator, or an API) and will intelligently decide which one to use to answer a complex query.'),\n",
       " Document(metadata={'source': 'rag.txt'}, page_content='By using LangChain, a developer can easily build a complete RAG application. They can use LangChain to load documents, create embeddings for them, store those embeddings in a vector database, set up the retriever, and chain it all together with an LLM to answer questions. This modular approach accelerates development and makes building complex, data-aware AI applications more accessible than ever before.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e205fe3",
   "metadata": {},
   "source": [
    "we can also perform this process with the simple `open` in python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfec114",
   "metadata": {},
   "source": [
    "## 6.2 Create a Vector Database\n",
    "Create a Chroma Database using Gemini embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fad57673",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(chunked_docs, embeddings, persist_directory=\"./chroma_db\")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416660dd",
   "metadata": {},
   "source": [
    "## 6.3 Advanced Prompt for Retrieval with Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79e4da2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Answer the question based ONLY on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in 3-4 sentences. Use citations like [source1], [source2].\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "263f65f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82bbf92",
   "metadata": {},
   "source": [
    "## 6.4 Query with Advanced RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17fbca07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Answer:\n",
      " Embeddings translate human language into numerical representations that machines can understand [Embeddings: The Language of Machines üß†].  This allows for vector search, enabling efficient retrieval of relevant information from a knowledge base by comparing the embedding of a query to the embeddings of documents [1].  The similarity of embeddings reflects the semantic meaning of the text, capturing the \"vibe\" of the information [An embedding is a vector].  This facilitates more accurate and nuanced information retrieval compared to traditional keyword-based methods.\n",
      "\n",
      "Sources:\n",
      " - {'source': 'rag.txt'}\n",
      " - {'source': 'rag.txt'}\n",
      " - {'source': 'rag.txt'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the advantages of using embeddings?\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(\"Answer:\\n\", result[\"result\"])\n",
    "print(\"\\nSources:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(\" -\", doc.metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c293a",
   "metadata": {},
   "source": [
    "## 6.5 Evaluating RAG answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f88d9f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation:\n",
      " [{'results': 'GRADE: CORRECT'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain\n",
    "\n",
    "examples = [\n",
    "    {\"query\": query, \"answer\": \"Translates data from human readable form to numerical form.\"}\n",
    "]\n",
    "\n",
    "predictions = [{\"query\": \"What is the main topic of the document?\", \"result\": result[\"result\"]}]\n",
    "\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "graded = eval_chain.evaluate(examples, predictions)\n",
    "\n",
    "print(\"Evaluation:\\n\", graded)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
